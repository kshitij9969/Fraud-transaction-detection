{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IEEEFraudDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwOvTsWchJZc",
        "colab_type": "code",
        "outputId": "dddfe760-793e-4a0f-d157-027b2f112e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# In this section, I have imported all the required libraries.\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Standard plotly imports\n",
        "#import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "#import cufflinks\n",
        "#import cufflinks as cf\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "\n",
        "# Using plotly + cufflinks in offline mode\n",
        "init_notebook_mode(connected=True)\n",
        "#cufflinks.go_offline(connected=True)\n",
        "\n",
        "# Preprocessing, modelling and evaluating\n",
        "from sklearn import preprocessing\n",
        "# Metrics for evaluating the performance of the model\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "# Importing Kfold for validation of the model\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
        "# Importing the classifier for classfying the frauds and valid transactions\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "## Hyperopt modules\n",
        "# Hyperopt modules are used to perform bayesian optimization which is a probabilitic model approach to find\n",
        "# the minimum of a function.\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
        "# Used for working with functions and other callable objects to extend their abilities without having to rewrite it again\n",
        "from functools import partial \n",
        "# Importing the garbage collector\n",
        "import gc"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu1XH0nHgP3g",
        "colab_type": "code",
        "outputId": "626e6cdc-e2e8-4f7c-fd95-a020966517e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Mounting the drive and getting the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls '/content/drive/My Drive/ieee-fraud-detection'\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "sample_submission.csv  test_transaction.csv  Training_Data_Google.csv\n",
            "test_identity.csv      train_identity.csv    train_transaction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pua6oKMRmW_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this section, I am importing the dataset.\n",
        "transactions = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/train_transaction.csv')\n",
        "identity = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/train_identity.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rguFVq1DNNya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir-j8hAjhHqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this section, I create a few functions to help us.\n",
        "# This function provides the summary of the dataset we pass to it.\n",
        "def resumetable(df):\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
        "    summary = summary.reset_index()\n",
        "    summary['Name'] = summary['index']\n",
        "    summary = summary[['Name','dtypes']]\n",
        "    summary['Missing'] = df.isnull().sum().values    \n",
        "    summary['Uniques'] = df.nunique().values\n",
        "    summary['First Value'] = df.loc[0].values\n",
        "    summary['Second Value'] = df.loc[1].values\n",
        "    summary['Third Value'] = df.loc[2].values\n",
        "\n",
        "    for name in summary['Name'].value_counts().index:\n",
        "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n",
        "\n",
        "    return summary\n",
        "\n",
        "# This function reduces the memory used by the dataset by converting the data type of the function\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "# This function removes the outlier. Outliers are values beyond mean+-(standard deviation)*3\n",
        "def CalcOutliers(df_num): \n",
        "\n",
        "    # calculating mean and std of the array\n",
        "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
        "\n",
        "    # seting the cut line to both higher and lower values\n",
        "    # You can change this value\n",
        "    cut = data_std * 3\n",
        "\n",
        "    #Calculating the higher and lower cut values\n",
        "    lower, upper = data_mean - cut, data_mean + cut\n",
        "\n",
        "    # creating an array of lower, higher and total outlier values \n",
        "    outliers_lower = [x for x in df_num if x < lower]\n",
        "    outliers_higher = [x for x in df_num if x > upper]\n",
        "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
        "\n",
        "    # array without outlier values\n",
        "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
        "    \n",
        "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
        "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
        "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
        "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
        "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aJitG18l2HL",
        "colab_type": "code",
        "outputId": "5a99fcd8-125b-4be6-9ec9-27efe1f054db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "df_trans = reduce_mem_usage(transactions)\n",
        "df_id = reduce_mem_usage(identity)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
            "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac-8Fsqni_4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del transactions\n",
        "del identity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13GUy6HCAQUa",
        "colab_type": "text"
      },
      "source": [
        "Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MORE203cARyy",
        "colab_type": "code",
        "outputId": "b3381d8c-6155-43f9-d088-ff54037fe553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "df_trans = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/train_transaction.csv')\n",
        "df_test_trans = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/test_transaction.csv')\n",
        "\n",
        "df_id = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/train_identity.csv')\n",
        "df_test_id = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/test_identity.csv')\n",
        "\n",
        "sample_submission = pd.read_csv('/content/drive/My Drive/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n",
        "\n",
        "df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
        "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)\n",
        "\n",
        "# y_train = df_train['isFraud'].copy()\n",
        "del df_trans, df_id, df_test_trans, df_test_id"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(590540, 434)\n",
            "(506691, 433)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MllVlWoYHNZM",
        "colab_type": "code",
        "outputId": "7eee9cd9-6053-4c8c-f89d-92c0b1bbd92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "df_train = reduce_mem_usage(df_train)\n",
        "df_test = reduce_mem_usage(df_test)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 645.97 Mb (67.0% reduction)\n",
            "Mem. usage decreased to 561.50 Mb (66.5% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSlMBpiyHQK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
        "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
        "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
        "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
        "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
        "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
        "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
        "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
        "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
        "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
        "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
        "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
        "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
        "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
        "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
        "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
        "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
        "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
        "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
        "\n",
        "us_emails = ['gmail', 'net', 'edu']\n",
        "\n",
        "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
        "for c in ['P_emaildomain', 'R_emaildomain']:\n",
        "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
        "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
        "    \n",
        "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
        "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
        "    \n",
        "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
        "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gZPeURJfgg",
        "colab_type": "code",
        "outputId": "1899635c-dbd2-4aeb-805b-e6453cc7d121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# df_train['id_03'].dtype\n",
        "# for f in df_train.drop('isFraud', axis=1).columns:\n",
        "#   type(f)\n",
        "\n",
        "# print(df_train['id_01'])\n",
        "print(df_test['id-01'])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        -45.0\n",
            "1          0.0\n",
            "2         -5.0\n",
            "3        -45.0\n",
            "4        -95.0\n",
            "          ... \n",
            "506686     NaN\n",
            "506687     NaN\n",
            "506688     NaN\n",
            "506689     NaN\n",
            "506690     NaN\n",
            "Name: id-01, Length: 506691, dtype: float16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9COcY3WzH40D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for f in df_train.drop('isFraud', axis=1).columns:\n",
        "    if df_train[f].dtype == 'object': \n",
        "        lbl = preprocessing.LabelEncoder()\n",
        "        lbl.fit(list(df_train[f].values))\n",
        "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
        "    \n",
        "# for f in df_test.columns:\n",
        "#      if df_test[f].dtype == 'object':\n",
        "#         lbl = preprocessing.LabelEncoder()\n",
        "#         lbl.fit(list(df_test[f].values))\n",
        "#         df_test[f] = lbl.transform(list(df_test[f].values))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-itR5avJ8zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\n",
        "df_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\n",
        "df_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\n",
        "df_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl6tzQJKJ_rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
        "df_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
        "df_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
        "df_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
        "\n",
        "df_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
        "df_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
        "df_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
        "df_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3OUJk8KDHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\n",
        "df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWgOMH_CKWt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['isFraud'] = 'test'\n",
        "df = pd.concat([df_train, df_test], axis=0, sort=False )\n",
        "df = df.reset_index()\n",
        "df = df.drop('index', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAnN_OdKX25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n",
        "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
        "\n",
        "    principalComponents = pca.fit_transform(df[cols])\n",
        "\n",
        "    principalDf = pd.DataFrame(principalComponents)\n",
        "\n",
        "    df.drop(cols, axis=1, inplace=True)\n",
        "\n",
        "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
        "\n",
        "    df = pd.concat([df, principalDf], axis=1)\n",
        "    \n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sca-Zx2QKiEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mas_v = df_train.columns[55:394]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX1Ilp2pKlkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "for col in mas_v:\n",
        "    df[col] = df[col].fillna((df[col].min() - 2))\n",
        "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
        "\n",
        "    \n",
        "df = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI_u8WpxKpj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtYObF7JLENq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = df_train.sort_values('TransactionDT')\n",
        "y_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
        "\n",
        "X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n",
        "                                                    #'Card_ID'\n",
        "                                                   ], \n",
        "                                                   axis=1)\n",
        "del df_train\n",
        "df_test = df_test[[\"TransactionDT\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KspV_YvEVHlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.to_csv(path_or_buf='/content/drive/My Drive/ieee-fraud-detection/FinalTrainingSet.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SosD36CCcrKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a1bbbf7-e913-4dea-bf87-c82ba4f1851e"
      },
      "source": [
        "for f in X_train.columns:\n",
        "  if f=='id-01':\n",
        "    print(True)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUXJfLj3fXMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9188f7e0-72c5-478a-9ca5-149157d4ccc9"
      },
      "source": [
        "X_train.memory_usage().sum()/1024**2"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "490.8208465576172"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Gh9fa8LLRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import plot_importance\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "import time\n",
        "def objective(params):\n",
        "    time1 = time.time()\n",
        "    params = {\n",
        "        'max_depth': int(params['max_depth']),\n",
        "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
        "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
        "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
        "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
        "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
        "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
        "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
        "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
        "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
        "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
        "    }\n",
        "\n",
        "    print(\"\\n############## New Run ################\")\n",
        "    print(f\"params = {params}\")\n",
        "    FOLDS = 7\n",
        "    count=1\n",
        "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
        "    y_preds = np.zeros(sample_submission.shape[0])\n",
        "    y_oof = np.zeros(X_train.shape[0])\n",
        "    score_mean = 0\n",
        "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
        "        clf = xgb.XGBClassifier(\n",
        "            n_estimators=600, random_state=4, verbose=True, \n",
        "            tree_method='gpu_hist', \n",
        "            **params\n",
        "        )\n",
        "\n",
        "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
        "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        clf.fit(X_tr, y_tr)\n",
        "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
        "        #print(y_pred_train)\n",
        "        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
        "        # plt.show()\n",
        "        score_mean += score\n",
        "        print(f'{count} CV - score: {round(score, 4)}')\n",
        "        count += 1\n",
        "    time2 = time.time() - time1\n",
        "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
        "    gc.collect()\n",
        "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
        "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
        "    return -(score_mean / FOLDS)\n",
        "\n",
        "\n",
        "space = {\n",
        "    # The maximum depth of a tree, same as GBM.\n",
        "    # Used to control over-fitting as higher depth will allow model \n",
        "    # to learn relations very specific to a particular sample.\n",
        "    # Should be tuned using CV.\n",
        "    # Typical values: 3-10\n",
        "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
        "    \n",
        "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
        "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
        "    # is logistic regression since you might need help with feature selection.\n",
        "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
        "    \n",
        "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
        "    # approach can be more useful in tree-models where zeroing \n",
        "    # features might not make much sense.\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
        "    \n",
        "    # eta: Analogous to learning rate in GBM\n",
        "    # Makes the model more robust by shrinking the weights on each step\n",
        "    # Typical final values to be used: 0.01-0.2\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "    \n",
        "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
        "    # fraction of columns to be randomly samples for each tree.\n",
        "    # Typical values: 0.5-1\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
        "    \n",
        "    # A node is split only when the resulting split gives a positive\n",
        "    # reduction in the loss function. Gamma specifies the \n",
        "    # minimum loss reduction required to make a split.\n",
        "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
        "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
        "    \n",
        "    # more increases accuracy, but may lead to overfitting.\n",
        "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
        "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
        "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
        "    \n",
        "    # specifies the minimum samples per leaf node.\n",
        "    # the minimum number of samples (data) to group into a leaf. \n",
        "    # The parameter can greatly assist with overfitting: larger sample\n",
        "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
        "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
        "    \n",
        "    # subsample: represents a fraction of the rows (observations) to be \n",
        "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
        "    # in their paper A Scalable Tree Boosting System recommend \n",
        "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
        "    \n",
        "    # randomly select a fraction of the features.\n",
        "    # feature_fraction: controls the subsampling of features used\n",
        "    # for training (as opposed to subsampling the actual training data in \n",
        "    # the case of bagging). Smaller fractions reduce overfitting.\n",
        "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
        "    \n",
        "    # randomly bag or subsample training data.\n",
        "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
        "    \n",
        "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
        "    # of the training data. Both values need to be set for bagging to be used.\n",
        "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
        "    # fractions and frequencies reduce overfitting.\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZqdWRtLNth",
        "colab_type": "code",
        "outputId": "ac036fd3-ad53-4c21-f8be-2f7e27614328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "source": [
        "# Set algoritm parameters\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=27)\n",
        "\n",
        "# Print best parameters\n",
        "best_params = space_eval(space, best)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "############## New Run ################\n",
            "params = {'max_depth': 21, 'gamma': '0.233', 'subsample': '0.20', 'reg_alpha': '0.287', 'reg_lambda': '0.247', 'learning_rate': '0.164', 'num_leaves': '90.000', 'colsample_bytree': '0.650', 'min_child_samples': '170.000', 'feature_fraction': '0.547', 'bagging_fraction': '0.726'}\n",
            "  0%|          | 0/27 [00:00<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "XGBoostError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-ab88a656df40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             max_evals=27)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Print best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-139-4f8e63bef0e0>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m#y_pred_train = clf.predict_proba(X_vl)[:,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#print(y_pred_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXGBoostError\u001b[0m: [13:35:28] /workspace/src/tree/updater_gpu_hist.cu:1407: Exception in gpu_hist: NCCL failure :unhandled cuda error /workspace/src/tree/../common/device_helpers.cuh(896)\n\nStack trace:\n  [bt] (0) /usr/local/lib/python3.6/dist-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24) [0x7f17d0891cb4]\n  [bt] (1) /usr/local/lib/python3.6/dist-packages/xgboost/./lib/libxgboost.so(xgboost::tree::GPUHistMakerSpecialised<xgboost::detail::GradientPairInternal<double> >::Update(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, std::vector<xgboost::RegTree*, std::allocator<xgboost::RegTree*> > const&)+0x1270) [0x7f17d0acd7f0]\n  [bt] (2) /usr/local/lib/python3.6/dist-packages/xgboost/./lib/libxgboost.so(xgboost::gbm::GBTree::BoostNewTrees(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, int, std::vector<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> >, std::allocator<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> > > >*)+0xa81) [0x7f17d0917791]\n  [bt] (3) /usr/local/lib/python3.6/dist-packages/xgboost/./lib/libxgboost.so(xgboost::gbm::GBTree::DoBoost(xgboost::DMatrix*, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::ObjFunction*)+0xd65) [0x7f17d0918c95]\n  [bt] (4) /usr/local/lib/python3.6/dist-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x396) [0x7f17d092b556]\n  [bt] (5) /usr/local/lib/python3.6/dist-packages/xgboost/./lib/libxgboost.so(XGBoosterUpdateOneIter+0x35) [0x7f17d088eaa5]\n  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f181a059dae]\n  [bt] (7) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f181a05971f]\n  [bt] (8) /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2b4) [0x7f181a26d5c4]\n\n"
          ]
        }
      ]
    }
  ]
}